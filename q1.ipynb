{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7ac836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Softmax, GELU\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76264d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImageParams:\n",
    "    width: int\n",
    "    height: int\n",
    "    in_channel: int\n",
    "@dataclass\n",
    "class ModelParameters:\n",
    "    patch_size: int\n",
    "    inner_dim: int\n",
    "    transformer_layers: int\n",
    "    num_head: int\n",
    "    embed_dropout: float\n",
    "    attn_dropout: float\n",
    "    mlp_dropout: float\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int\n",
    "    out_classes: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ba783",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = ImageParams(width=32, height=32, in_channel=3)\n",
    "mparams = ModelParameters(patch_size=4, inner_dim=256, transformer_layers=6, num_head=4, embed_dropout=0.1, attn_dropout=0, mlp_dropout=0.1)\n",
    "hparams = Hyperparameters(batch_size=1024, out_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2dc5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = ImageParams(width=32, height=32, in_channel=3)\n",
    "mparams = ModelParameters(patch_size=4, inner_dim=256, transformer_layers=6, num_head=4, embed_dropout=0.1, attn_dropout=0, mlp_dropout=0.1)\n",
    "hparams = Hyperparameters(batch_size=1024, out_classes=10)\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, mparams, hparams, img_info):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = mparams.patch_size\n",
    "        self.img_size = img_info.width\n",
    "        self.num_patches = (self.img_size//self.patch_size) * (self.img_size//self.patch_size)\n",
    "        self.D = mparams.inner_dim\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=img_info.in_channel,\n",
    "            out_channels=self.D,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1,self.D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [B, C, H, W]\n",
    "        # Output: [B, N, D] here N is selected num_patches(from image) + 1 (cls token)\n",
    "        b = x.shape[0]\n",
    "        cls_token = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = self.patch_embed(x)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d')\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        return x\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super(MHA, self).__init__()\n",
    "        self.D = mparams.inner_dim\n",
    "        self.num_head = mparams.num_head\n",
    "        assert self.D % self.num_head == 0 , 'Inner dimensions and number of attention head need to be perfectly divisible'\n",
    "        self.head_size = self.D // self.num_head\n",
    "        self.all_head_size = self.head_size * self.num_head\n",
    "        # Set up QKV\n",
    "        self.query = nn.Linear(in_features=self.D, out_features=self.all_head_size)\n",
    "        self.key = nn.Linear(in_features=self.D, out_features=self.all_head_size)\n",
    "        self.value = nn.Linear(in_features=self.D, out_features=self.all_head_size)\n",
    "        self.output = nn.Linear(in_features=self.D, out_features=self.D)\n",
    "        self.attn_dropout = nn.Dropout(mparams.attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(mparams.attn_dropout)\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "    def forward(self, x, mask= None):\n",
    "        # Input: [B, N, D]\n",
    "        # For atten: [B, num_head, num_patches, head_size]\n",
    "        # Output: [B, N, D]\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        # For atten: [B, num_head, num_patches, head_size]\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_head)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.num_head)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.num_head)\n",
    "        attn_score = torch.matmul(q, k.transpose(-1, -2))/ self.head_size**0.5\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = self.softmax(attn_score)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "         # sum with V\n",
    "        context = torch.matmul(attn_probs,v) #[B,h,n,d]\n",
    "        # combine all heads\n",
    "        context = rearrange(context, 'b h n d -> b n (h d) ')\n",
    "        output = self.output(context)\n",
    "        output = self.proj_dropout(output)\n",
    "        return output\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super().__init__()\n",
    "        self.D = mparams.inner_dim\n",
    "        self.hidden_dim = 4* self.D\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.D, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(mparams.mlp_dropout),\n",
    "            nn.Linear(self.hidden_dim, self.D),\n",
    "            nn.Dropout(mparams.mlp_dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(mparams.inner_dim)\n",
    "        self.attn = MHA(mparams=mparams, hparams=hparams)\n",
    "        self.norm2 = nn.LayerNorm(mparams.inner_dim)\n",
    "        self.ffn = MLP(mparams=mparams, hparams=hparams)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x) + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x) + residual\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super().__init__()\n",
    "        self.depth = mparams.transformer_layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(mparams=mparams, hparams=hparams) for _ in range(self.depth)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, mparams, hparams, img_info):\n",
    "        super().__init__()\n",
    "        image_width = img_info.width\n",
    "        patch_size = mparams.patch_size\n",
    "        num_patches = (image_width//patch_size)**2\n",
    "        self.pos_embed = nn.Parameter(torch.rand(1, num_patches+1, mparams.inner_dim))\n",
    "        self.patch_embed = PatchEmbedding(mparams=mparams, hparams=hparams, img_info=img_info)\n",
    "        self.transformer = Transformer(mparams=mparams, hparams=hparams)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(mparams.inner_dim),\n",
    "            nn.Linear(mparams.inner_dim, hparams.out_classes)\n",
    "        )\n",
    "        self.embed_dropout = nn.Dropout(mparams.embed_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.embed_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        cls_token_ouput = x[:,0] # or u can do x.mean(dim=1) if we do a mean pooling for the final cls token\n",
    "        return self.mlp_head(cls_token_ouput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3d98f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tensor shape: torch.Size([2, 3, 32, 32])\n",
      "Output Shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.rand(2,3,32,32)\n",
    "print(f'test tensor shape: {test_tensor.shape}')\n",
    "vit = ViT(mparams=mparams, hparams=hparams, img_info=img_info)\n",
    "output = vit.forward(test_tensor)\n",
    "print(f'Output Shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010f256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
