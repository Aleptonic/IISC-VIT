{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7ac836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Softmax, GELU\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95a2fa",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76264d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImageParams:\n",
    "    width: int\n",
    "    height: int\n",
    "    in_channel: int\n",
    "@dataclass\n",
    "class ModelParameters:\n",
    "    patch_size: int\n",
    "    inner_dim: int\n",
    "    transformer_layers: int\n",
    "    num_head: int\n",
    "    embed_dropout: float\n",
    "    attn_dropout: float\n",
    "    mlp_dropout: float\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int\n",
    "    out_classes: int\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ba783",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = ImageParams(width=32, height=32, in_channel=3)\n",
    "mparams = ModelParameters(patch_size=4, inner_dim=256, transformer_layers=6, num_head=4, embed_dropout=0.1, attn_dropout=0, mlp_dropout=0.1)\n",
    "hparams = Hyperparameters(batch_size=1024, out_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_info = ImageParams(width=32, height=32, in_channel=3)\n",
    "mparams = ModelParameters(patch_size=4, inner_dim=192, transformer_layers=12, num_head=3, embed_dropout=0.1, attn_dropout=0, mlp_dropout=0.1)\n",
    "hparams = Hyperparameters(batch_size=1024, out_classes=10, epochs=2, learning_rate=5e-4*(1024/512), weight_decay=0.05)\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, mparams, hparams, img_info):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = mparams.patch_size\n",
    "        self.img_size = img_info.width\n",
    "        self.num_patches = (self.img_size//self.patch_size) * (self.img_size//self.patch_size)\n",
    "        self.D = mparams.inner_dim\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=img_info.in_channel,\n",
    "            out_channels=self.D,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1,self.D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [B, C, H, W]\n",
    "        # Output: [B, N, D] here N is selected num_patches(from image) + 1 (cls token)\n",
    "        b = x.shape[0]\n",
    "        cls_token = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = self.patch_embed(x)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d')\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        return x\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super(MHA, self).__init__()\n",
    "        self.D = mparams.inner_dim\n",
    "        self.num_head = mparams.num_head\n",
    "        assert self.D % self.num_head == 0 , 'Inner dimensions and number of attention head need to be perfectly divisible'\n",
    "        self.head_size = self.D // self.num_head\n",
    "        self.all_head_size = self.head_size * self.num_head\n",
    "        # Set up QKV\n",
    "        self.query = nn.Linear(in_features=self.D, out_features=self.all_head_size)\n",
    "        self.key = nn.Linear(in_features=self.D, out_features=self.all_head_size)\n",
    "        self.value = nn.Linear(in_features=self.D, out_features=self.all_head_size)\n",
    "        self.output = nn.Linear(in_features=self.D, out_features=self.D)\n",
    "        self.attn_dropout = nn.Dropout(mparams.attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(mparams.attn_dropout)\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "    def forward(self, x, mask= None):\n",
    "        # Input: [B, N, D]\n",
    "        # For atten: [B, num_head, num_patches, head_size]\n",
    "        # Output: [B, N, D]\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        # For atten: [B, num_head, num_patches, head_size]\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_head)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.num_head)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.num_head)\n",
    "        attn_score = torch.matmul(q, k.transpose(-1, -2))/ self.head_size**0.5\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = self.softmax(attn_score)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "         # sum with V\n",
    "        context = torch.matmul(attn_probs,v) #[B,h,n,d]\n",
    "        # combine all heads\n",
    "        context = rearrange(context, 'b h n d -> b n (h d) ')\n",
    "        output = self.output(context)\n",
    "        output = self.proj_dropout(output)\n",
    "        return output\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super().__init__()\n",
    "        self.D = mparams.inner_dim\n",
    "        self.hidden_dim = 4* self.D\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.D, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(mparams.mlp_dropout),\n",
    "            nn.Linear(self.hidden_dim, self.D),\n",
    "            nn.Dropout(mparams.mlp_dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(mparams.inner_dim)\n",
    "        self.attn = MHA(mparams=mparams, hparams=hparams)\n",
    "        self.norm2 = nn.LayerNorm(mparams.inner_dim)\n",
    "        self.ffn = MLP(mparams=mparams, hparams=hparams)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x) + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x) + residual\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, mparams, hparams):\n",
    "        super().__init__()\n",
    "        self.depth = mparams.transformer_layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(mparams=mparams, hparams=hparams) for _ in range(self.depth)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, mparams, hparams, img_info):\n",
    "        super().__init__()\n",
    "        image_width = img_info.width\n",
    "        patch_size = mparams.patch_size\n",
    "        num_patches = (image_width//patch_size)**2\n",
    "        self.pos_embed = nn.Parameter(torch.rand(1, num_patches+1, mparams.inner_dim))\n",
    "        self.patch_embed = PatchEmbedding(mparams=mparams, hparams=hparams, img_info=img_info)\n",
    "        self.transformer = Transformer(mparams=mparams, hparams=hparams)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(mparams.inner_dim),\n",
    "            nn.Linear(mparams.inner_dim, hparams.out_classes)\n",
    "        )\n",
    "        self.embed_dropout = nn.Dropout(mparams.embed_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.embed_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        cls_token_ouput = x[:,0] # or u can do x.mean(dim=1) if we do a mean pooling for the final cls token\n",
    "        return self.mlp_head(cls_token_ouput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3d98f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tensor shape: torch.Size([2, 3, 32, 32])\n",
      "Output Shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.rand(2,3,32,32)\n",
    "print(f'test tensor shape: {test_tensor.shape}')\n",
    "vit = ViT(mparams=mparams, hparams=hparams, img_info=img_info)\n",
    "output = vit.forward(test_tensor)\n",
    "print(f'Output Shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a47322",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ea8f6",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import random\n",
    "import numpy as np\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, scaler, lr_scheduler, epochs):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.mps.is_available() else 'cpu')\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = optimizer\n",
    "        self.scaler = scaler\n",
    "        self.lr_sch = lr_scheduler\n",
    "        self.epochs = epochs\n",
    "    def _train_one_epoch(self):\n",
    "        model = self.model\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for image, label in tqdm(self.train_loader, desc='Training'):\n",
    "            img, label = image.to(self.device), label.to(self.device)\n",
    "            # Trying automatic mixed precision (AMP)\n",
    "            if self.device =='cuda':\n",
    "                with torch.amp.autocast():\n",
    "                    logits = model(img)\n",
    "                    loss = self.criterion(logits, label)\n",
    "            # Backpropagation\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "            # Update lr\n",
    "            self.lr_sch.step()\n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_samples += label.size(0)\n",
    "            total_correct += (predicted == label).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "        return avg_loss, accuracy\n",
    "    def _validate(self):\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc=\"Validating\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                # Note: No need for AMP here, but it's fine if used\n",
    "                outputs = model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                # Metrics\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "        return avg_loss, accuracy\n",
    "    def _run_trainer(self, model_path):\n",
    "        model = self.model\n",
    "        model.to(self.device)\n",
    "        best_val_accuracy = 0.0\n",
    "        print(\"Starting Training...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"--- Epoch {epoch+1}/{self.epochs} ---\")\n",
    "\n",
    "            train_loss, train_acc = self._train_one_epoch()\n",
    "            val_loss, val_acc = self._validate()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} Summary:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"  Current Learning Rate: {self.optim.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "            # Save the model if it has the best validation accuracy so far\n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"✅ New best model saved with accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "        print(\"--- Training Finished ---\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    @staticmethod\n",
    "    def set_seed(seed: int, strict: bool = True):\n",
    "        \"\"\"\n",
    "        Sets the seed for all relevant libraries for reproducibility.\n",
    "        Args:\n",
    "            seed (int): The seed value.\n",
    "            strict (bool): If True, enforces full determinism which may cause errors on MPS/CUDA if an operation is not supported.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Specific settings for CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            if strict:\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "        # Specific settings for MPS\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.manual_seed(seed)\n",
    "        # Enforce deterministic algorithms globally\n",
    "        if strict:\n",
    "            torch.use_deterministic_algorithms(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa3cf4",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import collections\n",
    "from torchvision.transforms.v2 import MixUp, CutMix\n",
    "\n",
    "if not hasattr(collections.abc, 'Sequence'):\n",
    "    collections.abc.Sequence = collections.Sequence\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, image_information, batch_size):\n",
    "        self.img_info = image_information\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _train_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomResizedCrop((self.img_info.width, self.img_info.height), scale=(0.8, 1.0)),\n",
    "            # num_ops = number of augmentations to apply\n",
    "            # magnitude = strength of augmentations\n",
    "            transforms.RandAugment(num_ops=2, magnitude=9), # can be tweaked after checking how similar the images are\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "    def _val_test_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                         download=True, transform=self._train_transform())\n",
    "\n",
    "        val_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=self._val_test_transform())\n",
    "\n",
    "        # crave validation from train\n",
    "        num_train = len(train_dataset)\n",
    "        indices = list(range(num_train))\n",
    "        #torch.manual_seed(42) # write a better code so that across multiple runs we keep a seed\n",
    "        torch.utils.data.sampler.SubsetRandomSampler(indices)\n",
    "        split = int(0.9 * num_train)\n",
    "        train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "        # Create subsets based on the indices\n",
    "        train_subset = Subset(train_dataset, train_indices)\n",
    "        val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "        # Now, create the test dataset\n",
    "        test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=self._val_test_transform())\n",
    "\n",
    "        # Special Collate function for MixUp and CutMix\n",
    "        # These operations need to be applied to a whole batch at once.\n",
    "        mixup_cutmix = [\n",
    "            MixUp(alpha=1.0, num_classes=10),\n",
    "            CutMix(alpha=1.0, num_classes=10)\n",
    "        ]\n",
    "        combine_fn = lambda batch: mixup_cutmix[torch.randint(0,2,(1,)).item()](*torch.utils.data.default_collate(batch))\n",
    "\n",
    "\n",
    "        # Create the DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=self.batch_size,\n",
    "                                  shuffle=True, num_workers=2, collate_fn=combine_fn)\n",
    "\n",
    "        val_loader = DataLoader(val_subset, batch_size=self.batch_size,\n",
    "                                shuffle=False, num_workers=2)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size,\n",
    "                                 shuffle=False, num_workers=2)\n",
    "\n",
    "        print(\"DataLoaders created successfully.\")\n",
    "        print(f\"Training samples: {len(train_subset)}\")\n",
    "        print(f\"Validation samples: {len(val_subset)}\")\n",
    "        print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "# --- How to use it ---\n",
    "# data_handler = DataHandler(image_information=img_info, batch_size=1024)\n",
    "# train_loader, val_loader, test_loader = data_handler.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, test_loader, device, output_dir):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.class_names = self.test_loader.dataset.classes\n",
    "\n",
    "    def plot_confusion_matrix(self, cm, file_path):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(file_path)\n",
    "        plt.show() # Display in Colab\n",
    "\n",
    "    def evaluate(self, model_path):\n",
    "        \"\"\"Loads the best model and computes final metrics.\"\"\"\n",
    "        print(f\"\\n--- Starting Final Evaluation ---\")\n",
    "        # Load the best model state\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc=\"Testing\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Calculate Metrics ---\n",
    "        # Overall Accuracy\n",
    "        overall_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Overall Test Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Classification Report (Precision, Recall, F1-score)\n",
    "        report = classification_report(all_labels, all_preds,\n",
    "                                       target_names=self.class_names, output_dict=True)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        # --- Save Results ---\n",
    "        # Save classification report to a JSON file\n",
    "        report_path = os.path.join(self.output_dir, \"classification_report.json\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "        print(f\"Classification report saved to {report_path}\")\n",
    "\n",
    "        # Plot and save confusion matrix\n",
    "        cm_path = os.path.join(self.output_dir, \"confusion_matrix.png\")\n",
    "        self.plot_confusion_matrix(cm, cm_path)\n",
    "        print(f\"Confusion matrix plot saved to {cm_path}\")\n",
    "\n",
    "        return {\"overall_accuracy\": overall_accuracy, \"report\": report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def main(num_runs, master_seed, test_name, image_information, model_parameters, hyperparameters):\n",
    "    NUM_RUNS = num_runs\n",
    "    \"\"\" Checking MASTER_SEED\n",
    "    1. MASTER_SEED = 42  --> used for trial runs (T0)\n",
    "    \"\"\"\n",
    "    MASTER_SEED = master_seed\n",
    "    for i in range(NUM_RUNS):\n",
    "        run_seed = i+MASTER_SEED\n",
    "        Trainer.set_seed(seed=run_seed)\n",
    "        print(f\"\\n--- Starting Run {i+1}/{NUM_RUNS} (Seed: {run_seed}) ---\")\n",
    "\n",
    "        run_name = test_name+ f'_{i+1}'\n",
    "        run_output_dir = f\"run_outputs/{run_name}\"\n",
    "        local_model_path = os.path.join(run_output_dir, \"best_model.pth\")\n",
    "        eval_results_dir = os.path.join(run_output_dir, \"evaluation_results\")\n",
    "        os.makedirs(run_output_dir, exist_ok=True)\n",
    "        os.makedirs(eval_results_dir, exist_ok=True)\n",
    "\n",
    "        img_info = image_information\n",
    "        mparams = model_parameters\n",
    "        hparams = hyperparameters\n",
    "\n",
    "        train_loader, val_loader, test_loader = DataHandler(image_information=img_info, batch_size=hparams.batch_size)\n",
    "\n",
    "        base_model = ViT(mparams=mparams, hparams=hparams, img_info=img_info)\n",
    "        base_optimizer = optim.AdamW(params=base_model.parameters(), lr=hparams.learning_rate, weight_decay=hparams.weight_decay)\n",
    "        base_scaler = torch.amp.GradScaler(device='cuda')\n",
    "        base_scheduler = OneCycleLR(optimizer=base_optimizer, max_lr=hparams.learning_rate, steps_per_epoch=len(train_loader), epochs=hparams.epochs)\n",
    "\n",
    "        trainer = Trainer(model=base_model, train_loader=train_loader, val_loader=val_loader, optimizer=base_optimizer, scaler=base_scaler, lr_scheduler=base_scheduler)\n",
    "        trainer._run_trainer(model_path=local_model_path)\n",
    "        print(\"\\n--- Training complete. Starting final evaluation on test set. ---\")\n",
    "        evaluator = Evaluator(\n",
    "        model=base_model,\n",
    "        test_loader=test_loader,\n",
    "        device=trainer.device, \n",
    "        output_dir=eval_results_dir)\n",
    "        final_metrics = evaluator.evaluate(model_path=local_model_path)\n",
    "\n",
    "\n",
    "main(\n",
    "    num_runs=3,\n",
    "    master_seed=42,\n",
    "    test_name=f'test_run_check_T0',\n",
    "    image_information=ImageParams(width=32, height=32, in_channel=3),\n",
    "    model_parameters=ModelParameters(patch_size=4, inner_dim=192, transformer_layers=12, num_head=3, embed_dropout=0.1, attn_dropout=0.0, mlp_dropout=0.1),\n",
    "    hyperparameters=Hyperparameters(batch_size=1024, out_classes=10, epochs=100, learning_rate=5e-4*(1024/512), weight_decay=0.05)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
