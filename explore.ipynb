{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62ee5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 50000\n",
      "Training subset size: 45000\n",
      "Validation subset size: 5000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "# -- no augmentations --\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "# 1. Download the full training dataset\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "# 2. Split the full training dataset into a training and a validation set\n",
    "train_size = int(0.9 * len(full_train_dataset)) # 90% for training\n",
    "val_size = len(full_train_dataset) - train_size # 10% for validation\n",
    "train_subset, val_subset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Total training images: {len(full_train_dataset)}\")\n",
    "print(f\"Training subset size: {len(train_subset)}\")\n",
    "print(f\"Validation subset size: {len(val_subset)}\")\n",
    "\n",
    "\n",
    "# 3. Download the test dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f66685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: (32, 32)\n",
      "Class label: frog\n",
      "Saved one image as 'cifar_image1.png'\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "# Define the inverse of your normalization transform\n",
    "# If Normalize was ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), the inverse is the same.\n",
    "# T(x) = (x - mean) / std  =>  x = T(x) * std + mean\n",
    "un_normalize = transforms.Normalize(\n",
    "   mean=[-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5],\n",
    "   std=[1 / 0.5, 1 / 0.5, 1 / 0.5]\n",
    ")\n",
    "\n",
    "\n",
    "# Get the first image and its label\n",
    "image_tensor, label = full_train_dataset[0]\n",
    "print(f\"Original tensor shape: {image_tensor.size}\")\n",
    "print(f\"Class label: {full_train_dataset.classes[label]}\")\n",
    "\n",
    "# Un-normalize the tensor before saving\n",
    "image_to_save = un_normalize(to_tensor(image_tensor))\n",
    "\n",
    "# Save the image\n",
    "save_image(image_to_save, 'cifar_image1.png')\n",
    "print(\"Saved one image as 'cifar_image1.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cee31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = 'data/cifar_image1.png'\n",
    "test_img = Image.open(test_path)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "@dataclass\n",
    "class DataParameter:\n",
    "    img_size: int = 32\n",
    "    in_channel: int = 3\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    patch_size: int = 4\n",
    "    D: int = 256# this is the hidden_dimension of Xmer\n",
    "    batch_size: int = 1024 # this is the batch size\n",
    "    num_attn_head: int = 4\n",
    "    transformer_layers: int = 6\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits an image into patches and embeds them.\n",
    "    Drop out is missing\n",
    "    \"\"\"\n",
    "    def __init__(self, image_data = DataParameter(), hparams = Hyperparameters()):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.patch_size = hparams.patch_size\n",
    "        self.n_patches = (image_data.img_size // hparams.patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1,hparams.D))\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=image_data.in_channel,\n",
    "            out_channels=hparams.D,\n",
    "            kernel_size=hparams.patch_size,\n",
    "            stride=hparams.patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor with shape [B, C, H, W].\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded patches with shape [B, N, D].\n",
    "        \"\"\"\n",
    "        # x shape: [B, C, H, W] -> [B, D, H/P, W/P]\n",
    "        # Example: [B, 3, 32, 32] -> [B, 256, 8, 8]\n",
    "        b = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d')\n",
    "        cls_token = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = torch.cat((cls_token, x), dim=1) # added the cls token which is a learnable param\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09473978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Softmax\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hparams = Hyperparameters()):\n",
    "        super(Attention, self).__init__()\n",
    "        assert hparams.D % hparams.num_attn_head == 0, \"Embedding dim (D) must be divisible by num_heads\"\n",
    "        self.hparams = hparams\n",
    "        self.num_attn_head = hparams.num_attn_heads\n",
    "        self.attn_head_size = hparams.D // hparams.num_attn_head\n",
    "        self.all_head_size = self.num_attn_head * self.attn_head_size\n",
    "\n",
    "        self.query = nn.Linear(hparams.D, self.all_head_size)\n",
    "        self.key = nn.Linear(hparams.D, self.all_head_size)\n",
    "        self.value = nn.Linear(hparams.D, self.all_head_size)\n",
    "        self.output = nn.Linear(hparams.D, hparams.D)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.proj_dropout = nn.Dropout(0.0)\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "    def forward(self, x, mask=None):\n",
    "        # x = [B, N, D] N= num_pathces + 1 (coming from cls token )\n",
    "        q = self.query(x)\n",
    "        k = self.query(x)\n",
    "        v = self.query(x)\n",
    "        # reshape for multi head processing [B, N, D] -> [B, num_heads, N, head_dim]\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_attn_head)\n",
    "        k = rearrange(q, 'b n (h d) -> b h n d', h=self.num_attn_head)\n",
    "        v = rearrange(q, 'b n (h d) -> b h n d', h=self.num_attn_head)\n",
    "        # Reshape by transposing and then do dot product for attention\n",
    "        # attention has shape [B, h, N, N]\n",
    "        attn_score = torch.matmul(q, k.transpose(-1,-2)) / (self.attn_head_size**0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = self.softmax(attn_scores)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        # sum with V\n",
    "        context = torch.matmul(attn_probs,v) #[B,h,n,d]\n",
    "        # combine all heads\n",
    "        context = rearrange(context, 'b h n d -> b n (h d)')\n",
    "        output = self.output(context)\n",
    "        output = self.proj_dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import GELU\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dropout, hparams=Hyperparameters()):\n",
    "        super(MLP, self).__init__()\n",
    "        self.D = hparams.D\n",
    "        self.hidden_dim = 4 * self.D\n",
    "        self.dropout = dropout\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=self.D, out_features=self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(in_features=self.hidden_dim, out_features=self.D),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # [B, 65, D] coming from attention\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hparams=Hyperparameters()):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.D = hparams.D\n",
    "        self.norm1 = nn.LayerNorm(self.D)\n",
    "        self.attn = Attention(hparams=hparams)\n",
    "        self.norm2 = nn.LayerNorm(self.D)\n",
    "        self.ffn = MLP(dropout=0.1, hparams=hparams)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x) + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x) + residual\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hparams=Hyperparameters()):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.depth = hparams.transformer_layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(hparams=hparams) for _ in range(self.depth)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
